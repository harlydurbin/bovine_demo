usage: snakemake [-h] [--dry-run] [--profile PROFILE]
                 [--cache RULE [RULE ...]] [--snakefile FILE] [--cores [N]]
                 [--local-cores N] [--resources [NAME=INT [NAME=INT ...]]]
                 [--set-threads [RULE=THREADS [RULE=THREADS ...]]]
                 [--default-resources [NAME=INT [NAME=INT ...]]]
                 [--config [KEY=VALUE [KEY=VALUE ...]]]
                 [--configfile FILE [FILE ...]] [--directory DIR] [--touch]
                 [--keep-going] [--force] [--forceall]
                 [--forcerun [TARGET [TARGET ...]]]
                 [--prioritize TARGET [TARGET ...]]
                 [--batch RULE=BATCH/BATCHES] [--until TARGET [TARGET ...]]
                 [--omit-from TARGET [TARGET ...]] [--rerun-incomplete]
                 [--shadow-prefix DIR] [--report [HTMLFILE]]
                 [--export-cwl FILE] [--list] [--list-target-rules] [--dag]
                 [--rulegraph] [--filegraph] [--d3dag] [--summary]
                 [--detailed-summary] [--archive FILE]
                 [--cleanup-metadata FILE [FILE ...]] [--cleanup-shadow]
                 [--skip-script-cleanup] [--unlock] [--list-version-changes]
                 [--list-code-changes] [--list-input-changes]
                 [--list-params-changes] [--list-untracked]
                 [--delete-all-output] [--delete-temp-output]
                 [--bash-completion] [--keep-incomplete] [--version]
                 [--reason] [--gui [PORT]] [--printshellcmds] [--debug-dag]
                 [--stats FILE] [--nocolor] [--quiet] [--print-compilation]
                 [--verbose] [--force-use-threads] [--allow-ambiguity]
                 [--nolock] [--ignore-incomplete] [--latency-wait SECONDS]
                 [--wait-for-files [FILE [FILE ...]]] [--notemp]
                 [--keep-remote] [--keep-target-files]
                 [--allowed-rules ALLOWED_RULES [ALLOWED_RULES ...]]
                 [--max-jobs-per-second MAX_JOBS_PER_SECOND]
                 [--max-status-checks-per-second MAX_STATUS_CHECKS_PER_SECOND]
                 [--restart-times RESTART_TIMES] [--attempt ATTEMPT]
                 [--wrapper-prefix WRAPPER_PREFIX]
                 [--default-remote-provider {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS}]
                 [--default-remote-prefix DEFAULT_REMOTE_PREFIX]
                 [--no-shared-fs] [--greediness GREEDINESS] [--no-hooks]
                 [--overwrite-shellcmd OVERWRITE_SHELLCMD] [--debug]
                 [--runtime-profile FILE] [--mode {0,1,2}]
                 [--show-failed-logs] [--log-handler-script FILE]
                 [--log-service {none,slack}]
                 [--cluster CMD | --cluster-sync CMD | --drmaa [ARGS]]
                 [--cluster-config FILE] [--immediate-submit]
                 [--jobscript SCRIPT] [--jobname NAME]
                 [--cluster-status CLUSTER_STATUS] [--drmaa-log-dir DIR]
                 [--kubernetes [NAMESPACE]]
                 [--kubernetes-env ENVVAR [ENVVAR ...]]
                 [--container-image IMAGE] [--tibanna]
                 [--tibanna-sfn TIBANNA_SFN] [--precommand PRECOMMAND]
                 [--use-conda] [--list-conda-envs] [--cleanup-conda]
                 [--conda-prefix DIR] [--create-envs-only] [--use-singularity]
                 [--singularity-prefix DIR] [--singularity-args ARGS]
                 [--use-envmodules]
                 [target [target ...]]

Snakemake is a Python based language and execution environment for GNU Make-
like workflows.

optional arguments:
  -h, --help            show this help message and exit

EXECUTION:
  target                Targets to build. May be rules or files.
  --dry-run, --dryrun, -n
                        Do not execute anything, and display what would be
                        done. If you have a very large workflow, use --dry-run
                        --quiet to just print a summary of the DAG of jobs.
  --profile PROFILE     Name of profile to use for configuring Snakemake.
                        Snakemake will search for a corresponding folder in
                        /etc/xdg/snakemake and /home/hjdzpd/.config/snakemake.
                        Alternatively, this can be an absolute or relative
                        path. The profile folder has to contain a file
                        'config.yaml'. This file can be used to set default
                        values for command line options in YAML format. For
                        example, '--cluster qsub' becomes 'cluster: qsub' in
                        the YAML file. Profiles can be obtained from
                        https://github.com/snakemake-profiles.
  --cache RULE [RULE ...]
                        Store output files of given rules in a central cache
                        given by the environment variable
                        $SNAKEMAKE_OUTPUT_CACHE. Likewise, retrieve output
                        files of the given rules from this cache if they have
                        been created before (by anybody writing to the same
                        cache), instead of actually executing the rules.
                        Output files are identified by hashing all steps,
                        parameters and software stack (conda envs or
                        containers) needed to create them.
  --snakefile FILE, -s FILE
                        The workflow definition in form of a
                        snakefile.Usually, you should not need to specify
                        this. By default, Snakemake will search for
                        'Snakefile', 'snakefile', 'workflow/Snakefile',
                        'workflow/snakefile' beneath the current working
                        directory, in this order. Only if you definitely want
                        a different layout, you need to use this parameter.
  --cores [N], --jobs [N], -j [N]
                        Use at most N cores in parallel. If N is omitted or
                        'all', the limit is set to the number of available
                        cores.
  --local-cores N       In cluster mode, use at most N cores of the host
                        machine in parallel (default: number of CPU cores of
                        the host). The cores are used to execute local rules.
                        This option is ignored when not in cluster mode.
  --resources [NAME=INT [NAME=INT ...]], --res [NAME=INT [NAME=INT ...]]
                        Define additional resources that shall constrain the
                        scheduling analogously to threads (see above). A
                        resource is defined as a name and an integer value.
                        E.g. --resources gpu=1. Rules can use resources by
                        defining the resource keyword, e.g. resources: gpu=1.
                        If now two rules require 1 of the resource 'gpu' they
                        won't be run in parallel by the scheduler.
  --set-threads [RULE=THREADS [RULE=THREADS ...]]
                        Overwrite thread usage of rules. This allows to fine-
                        tune workflow parallelization. In particular, this is
                        helpful to target certain cluster nodes by e.g.
                        shifting a rule to use more, or less threads than
                        defined in the workflow. Thereby, THREADS has to be a
                        positive integer, and RULE has to be the name of the
                        rule.
  --default-resources [NAME=INT [NAME=INT ...]], --default-res [NAME=INT [NAME=INT ...]]
                        Define default values of resources for rules that do
                        not define their own values. In addition to plain
                        integers, python expressions over inputsize are
                        allowed (e.g. '2*input.size').When specifying this
                        without any arguments (--default-resources), it
                        defines 'mem_mb=max(2*input.size, 1000)'
                        'disk_mb=max(2*input.size, 1000)', i.e., default disk
                        and mem usage is twice the input file size but at
                        least 1GB.
  --config [KEY=VALUE [KEY=VALUE ...]], -C [KEY=VALUE [KEY=VALUE ...]]
                        Set or overwrite values in the workflow config object.
                        The workflow config object is accessible as variable
                        config inside the workflow. Default values can be set
                        by providing a JSON file (see Documentation).
  --configfile FILE [FILE ...], --configfiles FILE [FILE ...]
                        Specify or overwrite the config file of the workflow
                        (see the docs). Values specified in JSON or YAML
                        format are available in the global config dictionary
                        inside the workflow. Multiple files overwrite each
                        other in the given order.
  --directory DIR, -d DIR
                        Specify working directory (relative paths in the
                        snakefile will use this as their origin).
  --touch, -t           Touch output files (mark them up to date without
                        really changing them) instead of running their
                        commands. This is used to pretend that the rules were
                        executed, in order to fool future invocations of
                        snakemake. Fails if a file does not yet exist. Note
                        that this will only touch files that would otherwise
                        be recreated by Snakemake (e.g. because their input
                        files are newer). For enforcing a touch, combine this
                        with --force, --forceall, or --forcerun. Note however
                        that you loose the provenance information when the
                        files have been created in realitiy. Hence, this
                        should be used only as a last resort.
  --keep-going, -k      Go on with independent jobs if a job fails.
  --force, -f           Force the execution of the selected target or the
                        first rule regardless of already created output.
  --forceall, -F        Force the execution of the selected (or the first)
                        rule and all rules it is dependent on regardless of
                        already created output.
  --forcerun [TARGET [TARGET ...]], -R [TARGET [TARGET ...]]
                        Force the re-execution or creation of the given rules
                        or files. Use this option if you changed a rule and
                        want to have all its output in your workflow updated.
  --prioritize TARGET [TARGET ...], -P TARGET [TARGET ...]
                        Tell the scheduler to assign creation of given targets
                        (and all their dependencies) highest priority.
                        (EXPERIMENTAL)
  --batch RULE=BATCH/BATCHES
                        Only create the given BATCH of the input files of the
                        given RULE. This can be used to iteratively run parts
                        of very large workflows. Only the execution plan of
                        the relevant part of the workflow has to be
                        calculated, thereby speeding up DAG computation. It is
                        recommended to provide the most suitable rule for
                        batching when documenting a workflow. It should be
                        some aggregating rule that would be executed only
                        once, and has a large number of input files. For
                        example, it can be a rule that aggregates over
                        samples.
  --until TARGET [TARGET ...], -U TARGET [TARGET ...]
                        Runs the pipeline until it reaches the specified rules
                        or files. Only runs jobs that are dependencies of the
                        specified rule or files, does not run sibling DAGs.
  --omit-from TARGET [TARGET ...], -O TARGET [TARGET ...]
                        Prevent the execution or creation of the given rules
                        or files as well as any rules or files that are
                        downstream of these targets in the DAG. Also runs jobs
                        in sibling DAGs that are independent of the rules or
                        files specified here.
  --rerun-incomplete, --ri
                        Re-run all jobs the output of which is recognized as
                        incomplete.
  --shadow-prefix DIR   Specify a directory in which the 'shadow' directory is
                        created. If not supplied, the value is set to the
                        '.snakemake' directory relative to the working
                        directory.

UTILITIES:
  --report [HTMLFILE]   Create an HTML report with results and statistics. If
                        no filename is given, report.html is the default.
  --export-cwl FILE     Compile workflow to CWL and store it in given FILE.
  --list, -l            Show available rules in given Snakefile.
  --list-target-rules, --lt
                        Show available target rules in given Snakefile.
  --dag                 Do not execute anything and print the directed acyclic
                        graph of jobs in the dot language. Recommended use on
                        Unix systems: snakemake --dag | dot | display
  --rulegraph           Do not execute anything and print the dependency graph
                        of rules in the dot language. This will be less
                        crowded than above DAG of jobs, but also show less
                        information. Note that each rule is displayed once,
                        hence the displayed graph will be cyclic if a rule
                        appears in several steps of the workflow. Use this if
                        above option leads to a DAG that is too large.
                        Recommended use on Unix systems: snakemake --rulegraph
                        | dot | display
  --filegraph           Do not execute anything and print the dependency graph
                        of rules with their input and output files in the dot
                        language. This is an intermediate solution between
                        above DAG of jobs and the rule graph. Note that each
                        rule is displayed once, hence the displayed graph will
                        be cyclic if a rule appears in several steps of the
                        workflow. Use this if above option leads to a DAG that
                        is too large. Recommended use on Unix systems:
                        snakemake --filegraph | dot | display
  --d3dag               Print the DAG in D3.js compatible JSON format.
  --summary, -S         Print a summary of all files created by the workflow.
                        The has the following columns: filename, modification
                        time, rule version, status, plan. Thereby rule version
                        contains the versionthe file was created with (see the
                        version keyword of rules), and status denotes whether
                        the file is missing, its input files are newer or if
                        version or implementation of the rule changed since
                        file creation. Finally the last column denotes whether
                        the file will be updated or created during the next
                        workflow execution.
  --detailed-summary, -D
                        Print a summary of all files created by the workflow.
                        The has the following columns: filename, modification
                        time, rule version, input file(s), shell command,
                        status, plan. Thereby rule version contains the
                        version the file was created with (see the version
                        keyword of rules), and status denotes whether the file
                        is missing, its input files are newer or if version or
                        implementation of the rule changed since file
                        creation. The input file and shell command columns are
                        self explanatory. Finally the last column denotes
                        whether the file will be updated or created during the
                        next workflow execution.
  --archive FILE        Archive the workflow into the given tar archive FILE.
                        The archive will be created such that the workflow can
                        be re-executed on a vanilla system. The function needs
                        conda and git to be installed. It will archive every
                        file that is under git version control. Note that it
                        is best practice to have the Snakefile, config files,
                        and scripts under version control. Hence, they will be
                        included in the archive. Further, it will add input
                        files that are not generated by by the workflow itself
                        and conda environments. Note that symlinks are
                        dereferenced. Supported formats are .tar, .tar.gz,
                        .tar.bz2 and .tar.xz.
  --cleanup-metadata FILE [FILE ...], --cm FILE [FILE ...]
                        Cleanup the metadata of given files. That means that
                        snakemake removes any tracked version info, and any
                        marks that files are incomplete.
  --cleanup-shadow      Cleanup old shadow directories which have not been
                        deleted due to failures or power loss.
  --skip-script-cleanup
                        Don't delete wrapper scripts used for execution
  --unlock              Remove a lock on the working directory.
  --list-version-changes, --lv
                        List all output files that have been created with a
                        different version (as determined by the version
                        keyword).
  --list-code-changes, --lc
                        List all output files for which the rule body (run or
                        shell) have changed in the Snakefile.
  --list-input-changes, --li
                        List all output files for which the defined input
                        files have changed in the Snakefile (e.g. new input
                        files were added in the rule definition or files were
                        renamed). For listing input file modification in the
                        filesystem, use --summary.
  --list-params-changes, --lp
                        List all output files for which the defined params
                        have changed in the Snakefile.
  --list-untracked, --lu
                        List all files in the working directory that are not
                        used in the workflow. This can be used e.g. for
                        identifying leftover files. Hidden files and
                        directories are ignored.
  --delete-all-output   Remove all files generated by the workflow. Use
                        together with --dry-run to list files without actually
                        deleting anything. Note that this will not recurse
                        into subworkflows. Write-protected files are not
                        removed. Nevertheless, use with care!
  --delete-temp-output  Remove all temporary files generated by the workflow.
                        Use together with --dry-run to list files without
                        actually deleting anything. Note that this will not
                        recurse into subworkflows.
  --bash-completion     Output code to register bash completion for snakemake.
                        Put the following in your .bashrc (including the
                        accents): `snakemake --bash-completion` or issue it in
                        an open terminal session.
  --keep-incomplete     Do not remove incomplete output files by failed jobs.
  --version, -v         show program's version number and exit

OUTPUT:
  --reason, -r          Print the reason for each executed rule.
  --gui [PORT]          Serve an HTML based user interface to the given
                        network and port e.g. 168.129.10.15:8000. By default
                        Snakemake is only available in the local network
                        (default port: 8000). To make Snakemake listen to all
                        ip addresses add the special host address 0.0.0.0 to
                        the url (0.0.0.0:8000). This is important if Snakemake
                        is used in a virtualised environment like Docker. If
                        possible, a browser window is opened.
  --printshellcmds, -p  Print out the shell commands that will be executed.
  --debug-dag           Print candidate and selected jobs (including their
                        wildcards) while inferring DAG. This can help to debug
                        unexpected DAG topology or errors.
  --stats FILE          Write stats about Snakefile execution in JSON format
                        to the given file.
  --nocolor             Do not use a colored output.
  --quiet, -q           Do not output any progress or rule information.
  --print-compilation   Print the python representation of the workflow.
  --verbose             Print debugging output.

BEHAVIOR:
  --force-use-threads   Force threads rather than processes. Helpful if shared
                        memory (/dev/shm) is full or unavailable.
  --allow-ambiguity, -a
                        Don't check for ambiguous rules and simply use the
                        first if several can produce the same file. This
                        allows the user to prioritize rules by their order in
                        the snakefile.
  --nolock              Do not lock the working directory
  --ignore-incomplete, --ii
                        Do not check for incomplete output files.
  --latency-wait SECONDS, --output-wait SECONDS, -w SECONDS
                        Wait given seconds if an output file of a job is not
                        present after the job finished. This helps if your
                        filesystem suffers from latency (default 5).
  --wait-for-files [FILE [FILE ...]]
                        Wait --latency-wait seconds for these files to be
                        present before executing the workflow. This option is
                        used internally to handle filesystem latency in
                        cluster environments.
  --notemp, --nt        Ignore temp() declarations. This is useful when
                        running only a part of the workflow, since temp()
                        would lead to deletion of probably needed files by
                        other parts of the workflow.
  --keep-remote         Keep local copies of remote input files.
  --keep-target-files   Do not adjust the paths of given target files relative
                        to the working directory.
  --allowed-rules ALLOWED_RULES [ALLOWED_RULES ...]
                        Only consider given rules. If omitted, all rules in
                        Snakefile are used. Note that this is intended
                        primarily for internal use and may lead to unexpected
                        results otherwise.
  --max-jobs-per-second MAX_JOBS_PER_SECOND
                        Maximal number of cluster/drmaa jobs per second,
                        default is 10, fractions allowed.
  --max-status-checks-per-second MAX_STATUS_CHECKS_PER_SECOND
                        Maximal number of job status checks per second,
                        default is 10, fractions allowed.
  --restart-times RESTART_TIMES
                        Number of times to restart failing jobs (defaults to
                        0).
  --attempt ATTEMPT     Internal use only: define the initial value of the
                        attempt parameter (default: 1).
  --wrapper-prefix WRAPPER_PREFIX
                        Prefix for URL created from wrapper directive
                        (default: https://github.com/snakemake/snakemake-
                        wrappers/raw/). Set this to a different URL to use
                        your fork or a local clone of the repository, e.g.,
                        use a git URL like
                        'git+file://path/to/your/local/clone@'.
  --default-remote-provider {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS}
                        Specify default remote provider to be used for all
                        input and output files that don't yet specify one.
  --default-remote-prefix DEFAULT_REMOTE_PREFIX
                        Specify prefix for default remote provider. E.g. a
                        bucket name.
  --no-shared-fs        Do not assume that jobs share a common file system.
                        When this flag is activated, Snakemake will assume
                        that the filesystem on a cluster node is not shared
                        with other nodes. For example, this will lead to
                        downloading remote files on each cluster node
                        separately. Further, it won't take special measures to
                        deal with filesystem latency issues. This option will
                        in most cases only make sense in combination with
                        --default-remote-provider. Further, when using
                        --cluster you will have to also provide --cluster-
                        status. Only activate this if you know what you are
                        doing.
  --greediness GREEDINESS
                        Set the greediness of scheduling. This value between 0
                        and 1 determines how careful jobs are selected for
                        execution. The default value (1.0) provides the best
                        speed and still acceptable scheduling quality.
  --no-hooks            Do not invoke onstart, onsuccess or onerror hooks
                        after execution.
  --overwrite-shellcmd OVERWRITE_SHELLCMD
                        Provide a shell command that shall be executed instead
                        of those given in the workflow. This is for debugging
                        purposes only.
  --debug               Allow to debug rules with e.g. PDB. This flag allows
                        to set breakpoints in run blocks.
  --runtime-profile FILE
                        Profile Snakemake and write the output to FILE. This
                        requires yappi to be installed.
  --mode {0,1,2}        Set execution mode of Snakemake (internal use only).
  --show-failed-logs    Automatically display logs of failed jobs.
  --log-handler-script FILE
                        Provide a custom script containing a function 'def
                        log_handler(msg):'. Snakemake will call this function
                        for every logging output (given as a dictionary
                        msg)allowing to e.g. send notifications in the form of
                        e.g. slack messages or emails.
  --log-service {none,slack}
                        Set a specific messaging service for logging
                        output.Snakemake will notify the service on errors and
                        completed execution.Currently only slack is supported.

CLUSTER:
  --cluster CMD, -c CMD
                        Execute snakemake rules with the given submit command,
                        e.g. qsub. Snakemake compiles jobs into scripts that
                        are submitted to the cluster with the given command,
                        once all input files for a particular job are present.
                        The submit command can be decorated to make it aware
                        of certain job properties (name, rulename, input,
                        output, params, wildcards, log, threads and
                        dependencies (see the argument below)), e.g.: $
                        snakemake --cluster 'qsub -pe threaded {threads}'.
  --cluster-sync CMD    cluster submission command will block, returning the
                        remote exitstatus upon remote termination (for
                        example, this should be usedif the cluster command is
                        'qsub -sync y' (SGE)
  --drmaa [ARGS]        Execute snakemake on a cluster accessed via DRMAA,
                        Snakemake compiles jobs into scripts that are
                        submitted to the cluster with the given command, once
                        all input files for a particular job are present. ARGS
                        can be used to specify options of the underlying
                        cluster system, thereby using the job properties name,
                        rulename, input, output, params, wildcards, log,
                        threads and dependencies, e.g.: --drmaa ' -pe threaded
                        {threads}'. Note that ARGS must be given in quotes and
                        with a leading whitespace.
  --cluster-config FILE, -u FILE
                        A JSON or YAML file that defines the wildcards used in
                        'cluster'for specific rules, instead of having them
                        specified in the Snakefile. For example, for rule
                        'job' you may define: { 'job' : { 'time' : '24:00:00'
                        } } to specify the time for rule 'job'. You can
                        specify more than one file. The configuration files
                        are merged with later values overriding earlier ones.
                        This option is deprecated in favor of using --profile,
                        see docs.
  --immediate-submit, --is
                        Immediately submit all jobs to the cluster instead of
                        waiting for present input files. This will fail,
                        unless you make the cluster aware of job dependencies,
                        e.g. via: $ snakemake --cluster 'sbatch --dependency
                        {dependencies}. Assuming that your submit script (here
                        sbatch) outputs the generated job id to the first
                        stdout line, {dependencies} will be filled with space
                        separated job ids this job depends on.
  --jobscript SCRIPT, --js SCRIPT
                        Provide a custom job script for submission to the
                        cluster. The default script resides as 'jobscript.sh'
                        in the installation directory.
  --jobname NAME, --jn NAME
                        Provide a custom name for the jobscript that is
                        submitted to the cluster (see --cluster). NAME is
                        "snakejob.{name}.{jobid}.sh" per default. The wildcard
                        {jobid} has to be present in the name.
  --cluster-status CLUSTER_STATUS
                        Status command for cluster execution. This is only
                        considered in combination with the --cluster flag. If
                        provided, Snakemake will use the status command to
                        determine if a job has finished successfully or
                        failed. For this it is necessary that the submit
                        command provided to --cluster returns the cluster job
                        id. Then, the status command will be invoked with the
                        job id. Snakemake expects it to return 'success' if
                        the job was successfull, 'failed' if the job failed
                        and 'running' if the job still runs.
  --drmaa-log-dir DIR   Specify a directory in which stdout and stderr files
                        of DRMAA jobs will be written. The value may be given
                        as a relative path, in which case Snakemake will use
                        the current invocation directory as the origin. If
                        given, this will override any given '-o' and/or '-e'
                        native specification. If not given, all DRMAA stdout
                        and stderr files are written to the current working
                        directory.

KUBERNETES:
  --kubernetes [NAMESPACE]
                        Execute workflow in a kubernetes cluster (in the
                        cloud). NAMESPACE is the namespace you want to use for
                        your job (if nothing specified: 'default'). Usually,
                        this requires --default-remote-provider and --default-
                        remote-prefix to be set to a S3 or GS bucket where
                        your . data shall be stored. It is further advisable
                        to activate conda integration via --use-conda.
  --kubernetes-env ENVVAR [ENVVAR ...]
                        Specify environment variables to pass to the
                        kubernetes job.
  --container-image IMAGE
                        Docker image to use, e.g., when submitting jobs to
                        kubernetes. By default, this is
                        'https://hub.docker.com/r/snakemake/snakemake', tagged
                        with the same version as the currently running
                        Snakemake instance. Note that overwriting this value
                        is up to your responsibility. Any used image has to
                        contain a working snakemake installation that is
                        compatible with (or ideally the same as) the currently
                        running version.

TIBANNA:
  --tibanna             Execute workflow on AWS cloud using Tibanna. This
                        requires --default-remote-prefix to be set to S3
                        bucket name and prefix (e.g.
                        'bucketname/subdirectory') where input is already
                        stored and output will be sent to. Using --tibanna
                        implies --default-resources is set as default.
                        Optionally, use --precommand to specify any
                        preparation command to run before snakemake command on
                        the cloud (inside snakemake container on Tibanna VM).
                        Also, --use-conda, --use-singularity, --config,
                        --configfile are supported and will be carried over.
  --tibanna-sfn TIBANNA_SFN
                        Name of Tibanna Unicorn step function (e.g.
                        tibanna_unicorn_monty).This works as serverless
                        scheduler/resource allocator and must be deployed
                        first using tibanna cli. (e.g. tibanna deploy_unicorn
                        --usergroup=monty --buckets=bucketname)
  --precommand PRECOMMAND
                        Any command to execute before snakemake command on AWS
                        cloud such as wget, git clone, unzip, etc. This is
                        used with --tibanna.Do not include input/output
                        download/upload commands - file transfer between S3
                        bucket and the run environment (container) is
                        automatically handled by Tibanna.

CONDA:
  --use-conda           If defined in the rule, run job in a conda
                        environment. If this flag is not set, the conda
                        directive is ignored.
  --list-conda-envs     List all conda environments and their location on
                        disk.
  --cleanup-conda       Cleanup unused conda environments.
  --conda-prefix DIR    Specify a directory in which the 'conda' and 'conda-
                        archive' directories are created. These are used to
                        store conda environments and their archives,
                        respectively. If not supplied, the value is set to the
                        '.snakemake' directory relative to the invocation
                        directory. If supplied, the `--use-conda` flag must
                        also be set. The value may be given as a relative
                        path, which will be extrapolated to the invocation
                        directory, or as an absolute path.
  --create-envs-only    If specified, only creates the job-specific conda
                        environments then exits. The `--use-conda` flag must
                        also be set.

SINGULARITY:
  --use-singularity     If defined in the rule, run job within a singularity
                        container. If this flag is not set, the singularity
                        directive is ignored.
  --singularity-prefix DIR
                        Specify a directory in which singularity images will
                        be stored.If not supplied, the value is set to the
                        '.snakemake' directory relative to the invocation
                        directory. If supplied, the `--use-singularity` flag
                        must also be set. The value may be given as a relative
                        path, which will be extrapolated to the invocation
                        directory, or as an absolute path.
  --singularity-args ARGS
                        Pass additional args to singularity.

ENVIRONMENT MODULES:
  --use-envmodules      If defined in the rule, run job within the given
                        environment modules, loaded in the given order. This
                        can be combined with --use-conda and --use-
                        singularity, which will then be only used as a
                        fallback for rules which don't define environment
                        modules.
